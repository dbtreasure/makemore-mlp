{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "g = 2147483647\n",
    "\n",
    "class MLPNgramClassifier(torch.nn.Module):\n",
    "    def __init__(self, seed=42, block_size=3, embedding_size=2, hidden_size=100, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.g = torch.Generator().manual_seed(seed)\n",
    "        self.C = torch.nn.Parameter(torch.randn((27, embedding_size), generator=self.g, device=device, requires_grad=True))\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(((block_size * embedding_size), hidden_size), generator=self.g, device=device, requires_grad=True))\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(hidden_size, generator=self.g, device=device, requires_grad=True))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn((hidden_size, 27), generator=self.g, device=device, requires_grad=True))\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(27, generator=self.g, device=device, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.C[x]\n",
    "        h = torch.tanh(emb.view(-1, (self.block_size * self.embedding_size)) @ self.W1 + self.b1) \n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "\n",
    "words = open('names.txt').read().splitlines()\n",
    "\n",
    "# build the vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "device = torch.device('cpu')\n",
    "X = torch.as_tensor(X).to(device)\n",
    "Y = torch.as_tensor(Y).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, Y)\n",
    "train_ratio = .8\n",
    "validation_ratio = .1\n",
    "\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * train_ratio)\n",
    "n_train_batch=32\n",
    "n_validation = int(n_total * validation_ratio)\n",
    "n_validation_batch=32\n",
    "n_test = n_total - n_train - n_validation\n",
    "\n",
    "train_data, validation_data, test_data = random_split(dataset, [n_train, n_validation, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=n_train_batch)\n",
    "validation_loader = DataLoader(validation_data, batch_size=n_validation_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "embedding_size = 2\n",
    "hidden_size = 100\n",
    "model = MLPNgramClassifier(g, block_size, embedding_size, hidden_size, device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "loss_fn = nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run StepByStep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_seed(g)\n",
    "sbs.to(device)\n",
    "\n",
    "sbs.set_loaders(train_loader, validation_loader, test_loader)\n",
    "\n",
    "sbs.set_tensorboard('mlp-ngram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best loss:  2.239733041691245\n",
      "Current best embedding size:  2\n",
      "Current best hidden size:  100\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.239867694414682\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.232458601555517\n",
      "Current best embedding size:  2\n",
      "Current best hidden size:  150\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.231543315409945\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.231534537058559\n",
      "Current best embedding size:  2\n",
      "Current best hidden size:  250\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.231130409708866\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.226823933077728\n",
      "Current best embedding size:  2\n",
      "Current best hidden size:  300\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.2267742929418493\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.223254302628913\n",
      "Current best embedding size:  2\n",
      "Current best hidden size:  400\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.2223362894245486\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.208125872205718\n",
      "Current best embedding size:  3\n",
      "Current best hidden size:  100\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.2135109478427517\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1909017174976237\n",
      "Current best embedding size:  3\n",
      "Current best hidden size:  150\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1970289793455917\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1839272981355435\n",
      "Current best embedding size:  3\n",
      "Current best hidden size:  250\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.188112594970981\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.183837354601182\n",
      "Current best embedding size:  3\n",
      "Current best hidden size:  300\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.188999831927274\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1677735082564817\n",
      "Current best embedding size:  4\n",
      "Current best hidden size:  150\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.18108452653818\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.164379039536352\n",
      "Current best embedding size:  4\n",
      "Current best hidden size:  200\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1779118062403224\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1594778096608995\n",
      "Current best embedding size:  4\n",
      "Current best hidden size:  250\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.169521828687542\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1553613466601216\n",
      "Current best embedding size:  4\n",
      "Current best hidden size:  300\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1768910826339294\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1532785829161862\n",
      "Current best embedding size:  4\n",
      "Current best hidden size:  400\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1624621024138464\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1504043001906235\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  250\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1664256176808103\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1478522875115047\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  300\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1638986136435125\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1334643621222336\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  350\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.152115803614238\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1322888664451924\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  500\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.146975283368607\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.1154141024820934\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  600\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.152157348135243\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.0870330649706292\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  750\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1366098900162322\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.0861798505629263\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  900\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1657562921291316\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.0849111719059574\n",
      "Current best embedding size:  5\n",
      "Current best hidden size:  950\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1861093183051987\n",
      "[][][][][][][][][][][]\n",
      "Current best loss:  2.0838253061701173\n",
      "Current best embedding size:  7\n",
      "Current best hidden size:  550\n",
      "Current best learning rate:  0.1\n",
      "Current validation loss:  2.1547446289370136\n",
      "[][][][][][][][][][][]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2y/lbpgx2bd4fq4xqwb9nr5kq980000gn/T/ipykernel_74640/3516043688.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensorboard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mlp-ngram'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0msbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/makemore-mlp/StepByStep.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, main_tag)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/makemore-mlp/StepByStep.py\u001b[0m in \u001b[0;36m_mini_batch\u001b[0;34m(self, validation, test)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mmini_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mmini_batch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/makemore-mlp/StepByStep.py\u001b[0m in \u001b[0;36mperform_train_step_fn\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_best_loss = 100\n",
    "for c_dim in range(2, 20):\n",
    "    for w_dim in range(100,1000, 50):\n",
    "        lr = .1\n",
    "        embedding_size = c_dim\n",
    "        hidden_size = w_dim\n",
    "        model = MLPNgramClassifier(g, block_size, embedding_size, hidden_size, device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
    "        reduced_optimizer = optim.SGD(model.parameters(), lr=lr/10, momentum=0.5)\n",
    "        further_reduced_optimizer = optim.SGD(model.parameters(), lr=lr/100, momentum=0.5)\n",
    "        loss_fn = nn.functional.cross_entropy\n",
    "\n",
    "        sbs = StepByStep(model, loss_fn, optimizer)\n",
    "        sbs.set_seed(g)\n",
    "        sbs.to(device)\n",
    "\n",
    "        sbs.set_loaders(train_loader, validation_loader, test_loader)\n",
    "\n",
    "        sbs.set_tensorboard('mlp-ngram')\n",
    "\n",
    "        sbs.train(10)\n",
    "        sbs.set_optimizer(reduced_optimizer)\n",
    "        sbs.train(5)\n",
    "        sbs.set_optimizer(further_reduced_optimizer)\n",
    "        sbs.train(5)\n",
    "        sbs.train_validation(1)\n",
    "        # compare sbs last losses loss with current_best_loss\n",
    "        if sbs.losses[-1] < current_best_loss:\n",
    "            current_best_loss = sbs.losses[-1]\n",
    "            print(\"Current best loss: \", current_best_loss)\n",
    "            print(\"Current best embedding size: \", embedding_size)\n",
    "            print(\"Current best hidden size: \", hidden_size)\n",
    "            print(\"Current best learning rate: \", lr)\n",
    "            print(\"Current validation loss: \", sbs.val_losses[-1])\n",
    "            print(\"[][][][][][][][][][][]\")\n",
    "            \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b52454ccac6a5061b9956402f9b0126832d9d09240c85a0ef34ec847cbca41c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
