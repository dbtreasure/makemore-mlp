{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "g = 2147483647\n",
    "\n",
    "class MLPNgramClassifier(torch.nn.Module):\n",
    "    def __init__(self, seed=42, block_size=3, embedding_size=2, hidden_size=100, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.g = torch.Generator().manual_seed(seed)\n",
    "        self.C = torch.nn.Parameter(torch.randn((27, embedding_size), generator=self.g, device=device, requires_grad=True))\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(((block_size * embedding_size), hidden_size), generator=self.g, device=device, requires_grad=True))\n",
    "        self.b1 = torch.nn.Parameter(torch.randn(hidden_size, generator=self.g, device=device, requires_grad=True))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn((hidden_size, 27), generator=self.g, device=device, requires_grad=True))\n",
    "        self.b2 = torch.nn.Parameter(torch.randn(27, generator=self.g, device=device, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.C[x]\n",
    "        h = torch.tanh(emb.view(-1, (self.block_size * self.embedding_size)) @ self.W1 + self.b1) \n",
    "        logits = h @ self.W2 + self.b2\n",
    "        return logits\n",
    "\n",
    "words = open('names.txt').read().splitlines()\n",
    "\n",
    "# build the vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "device = torch.device('cpu')\n",
    "X = torch.as_tensor(X).to(device)\n",
    "Y = torch.as_tensor(Y).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, Y)\n",
    "train_ratio = .8\n",
    "validation_ratio = .1\n",
    "\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * train_ratio)\n",
    "n_train_batch=32\n",
    "n_validation = int(n_total * validation_ratio)\n",
    "n_validation_batch=32\n",
    "n_test = n_total - n_train - n_validation\n",
    "\n",
    "train_data, validation_data, test_data = random_split(dataset, [n_train, n_validation, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=n_train_batch)\n",
    "validation_loader = DataLoader(validation_data, batch_size=n_validation_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "embedding_size = 2\n",
    "hidden_size = 100\n",
    "model = MLPNgramClassifier(g, block_size, embedding_size, hidden_size, device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "loss_fn = nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run StepByStep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs = StepByStep(model, loss_fn, optimizer)\n",
    "sbs.set_seed(g)\n",
    "sbs.to(device)\n",
    "\n",
    "sbs.set_loaders(train_loader, validation_loader, test_loader)\n",
    "\n",
    "sbs.set_tensorboard('mlp-ngram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[][][][][][][][][][][]\n",
      "c_dim:  2\n",
      "w_dim:  100\n",
      "momentum:  0.5\n",
      "Epoch 1 of 20...\n",
      "Epoch 2 of 20...\n",
      "Loss: 2.676296527242761\n",
      "Epoch 3 of 20...\n",
      "Loss: 2.517752488670142\n",
      "Epoch 4 of 20...\n",
      "Loss: 2.4760621981107502\n",
      "Epoch 5 of 20...\n",
      "Loss: 2.4499399249675733\n",
      "Epoch 6 of 20...\n",
      "Loss: 2.4306079607984628\n",
      "Epoch 7 of 20...\n",
      "Loss: 2.4149648614677104\n",
      "Epoch 8 of 20...\n",
      "Loss: 2.4015126001157734\n",
      "Epoch 9 of 20...\n",
      "Loss: 2.3900265412569714\n",
      "Epoch 10 of 20...\n",
      "Loss: 2.3803088547972777\n",
      "Epoch 11 of 20...\n",
      "Loss: 2.371916571054686\n",
      "Epoch 12 of 20...\n",
      "Loss: 2.3647310439324816\n",
      "Epoch 13 of 20...\n",
      "Loss: 2.35824189108351\n",
      "Epoch 14 of 20...\n",
      "Loss: 2.3520385744193826\n",
      "Epoch 15 of 20...\n",
      "Loss: 2.3464347839480864\n",
      "Epoch 16 of 20...\n",
      "Loss: 2.341570083799068\n",
      "Epoch 17 of 20...\n",
      "Loss: 2.3373915909007086\n",
      "Epoch 18 of 20...\n",
      "Loss: 2.333732388485197\n",
      "Epoch 19 of 20...\n",
      "Loss: 2.330346008002507\n",
      "Epoch 20 of 20...\n",
      "Loss: 2.327257219048904\n",
      "[][][][][][][][][][][]\n",
      "[][][][][][][][][][][]\n",
      "c_dim:  2\n",
      "w_dim:  100\n",
      "momentum:  0.6\n",
      "Epoch 1 of 20...\n",
      "Epoch 2 of 20...\n",
      "Loss: 2.689395064955674\n",
      "Epoch 3 of 20...\n",
      "Loss: 2.5303724453919063\n",
      "Epoch 4 of 20...\n",
      "Loss: 2.4859159834797886\n",
      "Epoch 5 of 20...\n",
      "Loss: 2.455194938232989\n",
      "Epoch 6 of 20...\n",
      "Loss: 2.4329275616338846\n",
      "Epoch 7 of 20...\n",
      "Loss: 2.4159019208639902\n",
      "Epoch 8 of 20...\n",
      "Loss: 2.4023870659618525\n",
      "Epoch 9 of 20...\n",
      "Loss: 2.3912142891577886\n",
      "Epoch 10 of 20...\n",
      "Loss: 2.3815713064408066\n"
     ]
    }
   ],
   "source": [
    "# write a for loop from 2 to 20\n",
    "for c_dim in range(2, 20):\n",
    "    for w_dim in range(100,500, 20):\n",
    "        # write a for loop from 0.5 to 2.0 in steps of .1\n",
    "        for momentum in np.arange(0.5, 2.0, .1):\n",
    "            print(\"[][][][][][][][][][][]\")\n",
    "            print(\"c_dim: \", c_dim)\n",
    "            print(\"w_dim: \", w_dim)\n",
    "            print(\"momentum: \", momentum)\n",
    "            lr = .1\n",
    "            embedding_size = c_dim\n",
    "            hidden_size = w_dim\n",
    "            model = MLPNgramClassifier(g, block_size, embedding_size, hidden_size, device)\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "            loss_fn = nn.functional.cross_entropy\n",
    "\n",
    "            sbs = StepByStep(model, loss_fn, optimizer)\n",
    "            sbs.set_seed(g)\n",
    "            sbs.to(device)\n",
    "\n",
    "            sbs.set_loaders(train_loader, validation_loader, test_loader)\n",
    "\n",
    "            sbs.set_tensorboard('mlp-ngram')\n",
    "\n",
    "            sbs.train(20)\n",
    "            print(\"[][][][][][][][][][][]\")\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b52454ccac6a5061b9956402f9b0126832d9d09240c85a0ef34ec847cbca41c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
